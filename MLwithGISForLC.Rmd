---
title: "Machine Learning for GIS and Land Cover Estimation "
subtitle: "Random Forest Model on GIS file (extracted by Blanca)"
author: "Christophe Bontemps (SIAP)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    toc: yes
    toc_float: true
    highlight: tango
    number_sections: no
    theme: lumen
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( message = FALSE, warning = FALSE, results =FALSE, echo = TRUE) 
```

# Installing the packages

```{r packages}
# File management
library(readxl)


# GIS packages
library(raster) ## for reading "RASTER" files
library(rgdal)  ## for reading "shapefiles"
library(sp)     ## for adjusting CRS in 
library(terra)  ## see https://www.neonscience.org/resources/learning-hub/tutorials/image-raster-data-r

# Tidy data management packages
library(dplyr)
library(data.table)
library(modelsummary)
library(ggcorrplot)

# Model fitting packages for ML
library(rpart)
library(caret)


# Plotting packages
library(ggplot2)
library(RColorBrewer)

# Nice presentation of results
library(knitr)
library(papeR)

# My colors:
SIAP.color <- "#0385a8"
SIAP.red <- "#eb4034"

```



# Importing CSV data from QGIS

```{r}
# data directly exported from  QGIS
 DataForModelQGIS <- read.csv("Data/Dataset_for_model_wkt.csv")

# Data with lat and long (created for R)
DataForModelLAT <- read.csv("Data/Dataset_for_model_WGS84-B.csv")

```


### Spatial representation of  points from QGIS  

```{r}

# Convert to sf object
library(sf)
data_sf <- st_as_sf(DataForModelQGIS, wkt = "geometry", crs = unique(DataForModelQGIS$SpatialRef))

# Reproject the sf object to EPSG:4326 (should create latitude values are in the correct range [-90, 90] degrees.)
# data_sf <- st_transform(data_sf, crs = st_crs(4326))


```

```{r}
LCpalette = c('brown', 'red', 'blue', 'darkgreen', 'darkblue' )
data_sf %>%
  mutate( Land = as.factor(class)) %>%
ggplot() +
geom_sf(aes(color = Land),  alpha = 0.5) +
  scale_color_manual(values= LCpalette, name = "Land type") +
  theme_minimal() +
  labs(title = "Map from CSV Data", x = "Longitude", y = "Latitude", color = "Category")
```
```{r}
# 
# # Correct the latitude values to be negative if they are positive
# data_sf$y <- ifelse(data_sf$y > 0, -data_sf$y, data_sf$y)
# 
# # Create a new sf object with the corrected coordinates
# data_sf_corrected <- st_as_sf(data_sf, coords = c("x", "y"), crs = st_crs(data_sf))
# 
# # Now reproject the corrected sf object to EPSG:4326
# data_sf_corrected <- st_transform(data_sf_corrected, crs = 4326)



```

#### With a basemap

```{r}
library(ggspatial)
library(prettymapr)

# Define the bounding box
bbox <- st_bbox(data_sf)


# Create a ggplot object with the basemap
ggplot() +
  annotation_map_tile(type = "osm") +
  geom_sf(data = data_sf, color = "red", size = 2) +
 coord_sf(xlim = c(bbox["xmin"], bbox["xmax"]), ylim = c(bbox["ymin"], bbox["ymax"])) +
#  coord_sf(xlim = MylimX, ylim = MylimY) +
  theme_minimal() +
  labs(title = "Map with Spatial Data Overlay", x = "Longitude", y = "Latitude")
```

### Spatial representation of  points from QGIS  with Latitude and longitude 


```{r}
library(leaflet)

# If you want to set your own colors manually:
pal <- colorFactor(
  #palette = c('red',  'green', 'purple', 'orange', 'blue'),
  palette = c('brown', 'red', 'blue', 'darkgreen', 'darkblue' ),
  domain = DataForModelLAT$class
)

# Base map using leaflet with your data frame
m <- leaflet() %>%
  addTiles() %>%
  addCircleMarkers(data = DataForModelLAT,label = 
                   label = c("Barez soil", "Built-up", "Reef", "Vegetation", "Water") ,
                   lng = ~long, 
                   lat = ~lat, 
                   color = ~pal(class),
                   radius = 5 )
m

```


```{r}
library(ggspatial)
library(ggmap)


# Define the bounding box for Vanuatu [min long, min lat, max long, max lat]
vanuatu_bbox <- c(168.30470, -17.72838, 168.31418 ,-17.71909 )

# # Get the map from Google Maps using ggmap
# vanuatu_map <- get_map(center = c(lon =168.3094 , lat = -17.72374 ),
#                        zoom = 8, maptype = "terrain")

# Get the map from OpenStreetMap using ggmap
vanuatu_map <- get_map(location = c(lon = mean(vanuatu_bbox[c(1, 3)]), lat = mean(vanuatu_bbox[c(2, 4)])), 
                       source = "osm", zoom = 8)

# Plot the map with points
ggmap(vanuatu_map) +
  geom_point(data = DataForModelLAT_vanuatu, aes(x = long, y = lat), color = "red") +
  theme_void()


# Plot the map with points
ggmap(vanuatu_map) +
  geom_point(data = DataForModelLAT, aes(x = long, y = lat), color = "red") +
  theme_void()
```


# Data Analysis for model training

```{r}
# Formatting and removing variables (QGIS version) 
# DataForModel <-DataForModelQGIS %>%
#   select(-c("VALUE", "x", "y" ,"SpatialRef", "geometry"))

# Formatting and removing variables (lat-long version) 
DataForModel <-DataForModelLAT %>%
  select(-c("VALUE","lat", "long"))

# Change class to right format (factor)
DataForModel$class <- as.factor(DataForModel$class)  

```

### Summary statistics

```{r}
# Number of obs. per class
counts <- DataForModel %>%
  count(class) %>%
  arrange(n)  # Sort counts in ascending order

# Reorder the categories based on counts
DataForModel$class<- factor(DataForModel$class, levels = counts$class)

# Graphic
DataForModel %>%
ggplot() + 
  geom_bar(aes(y = class), colour="white", fill = SIAP.color) +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))+ 
  labs(x = "", y = "") +
  ggtitle("Land types ")
```


```{r, results=TRUE}
datasummary_skim(DataForModel , type = "categorical"  )
```


```{r, results=TRUE, echo=TRUE}
datasummary_skim(DataForModel , type = "numeric"  )
```


# Explanatory variables analysis

## Maps 

>  This can be done later. Here we leave this to  QGIS  

```{r}
# library(OpenStreetMap)
# 
#  upperLeft = c(-max(DataForModel$lat), min(DataForModel$long))
# # 
#  lowerRight = c(min(DataForModel$lat), max(DataForModel$long))
# 
# base_map  = openmap(upperLeft, lowerRight, type="osm")
# 
#  plot(base_map)

```


```{r}
# map <- ggplot() +
#   geom_point(data = DataForModel,
#               aes(x = x, y = y, color = class),
#               size = .6) +
#  ggtitle("Points location on and X-Y grid") + 
#          labs(x=  "X-axis used as Longitude" ,
#               y = "Y-axis used as Latitude")+
#   theme_classic()
# 
# 
# map
```



```{r}
# We compute the correlation matrix of the covariates
corr_coef<-cor(DataForModel[,2:12], use = "p")

#And then plot it with nice options 
ggcorrplot(corr_coef, 
           type = "lower",         # lower triangle of the matrix only
           #hc.order = TRUE,        # variable sorted from highest to lowest
           outline.col = "white",  #Color options
           lab = TRUE)
```


The data preparation & validation methods ends here, so letâ€™s process to machine learning models.

# Machine learning
## Train and Validation data sets

```{r }
# Splits data into training and testing sets
set.seed(007)
# We'll randomly pick some observation from the full data set

trainIndex <- createDataPartition(DataForModel$class, p = .80, 
                                  list = FALSE, 
                                  times = 1)

# Creating the two data sets: 
train_data <- DataForModel[ trainIndex,]
validation_data  <- DataForModel[-trainIndex,]

```


We have `r nrow(train_data)` observations in the training data set (80\%) and `r nrow(validation_data)` observations in the validation data set (20\%). It is important to have a look at our train data set and check if it has the same characteristics, in particular for the categories distribution.  

```{r,  results= TRUE }
datasummary_skim(train_data , type = "categorical"  )
```

##  Missing values in the training data set

```{r}
# see https://rpubs.com/NguyenKhanh20/1069336

anyNA(train_data)
```

> We do **not**  have missing values for any variables. 


##  Selecting Cross Validation parameters

```{r, include = FALSE, echo = FALSE}
# function to set up random seeds when running on several cores 
#  (you do not need to understand that)

setSeeds <- function(method = "cv", numbers = 1, repeats = 1, tunes = NULL, seed = 2512) {
  #B is the number of resamples and integer vector of M (numbers + tune length if any)
  B <- if (method == "cv") numbers
  else if(method == "repeatedcv") numbers * repeats
  else NULL
  
  if(is.null(length)) {
    seeds <- NULL
  } else {
    set.seed(seed = seed)
    seeds <- vector(mode = "list", length = B)
    seeds <- lapply(seeds, function(x) sample.int(n = 1000000, size = numbers + ifelse(is.null(tunes), 0, tunes)))
    seeds[[length(seeds) + 1]] <- sample.int(n = 1000000, size = 1)
  }
  # return seeds
  seeds
}

```

```{r}
# Summary function with six statistics of interest 
# sixStats <- function(...) c(twoClassSummary(...), 
#                             defaultSummary(...))
```

By default, repeated K-fold cross-validation is used here. The function `r `trainControl` can be used to specify the type of resampling. We use here K= 5 and 10 repetition of the process. We then estimate 5 x 10 = 50 different predictions.

```{r controls}

# control variables (see later)
K <- 5
Myrepeats <- 10
rcvTunes <- 1 # tune number of models
Myseed <- 2512

```


##  Random Forest

We begin with fitting a Random Forest model using cross-validation on the *Training* data set 

Random Forest is a *bagging* (*bootstrap aggregation*) method. During training a bootstrap sample is drawn from the training data together with a subset of the total amount of variables, we then pick the best available variable to split the tree into two daughter nodes repeatedly until we reach a stopping criterion. This process is repeated until we have trained as many individual decision trees as we want. We can then combine the output of the decision trees into one final output.

Our final output will be based on the **majority vote** of the individual decision trees. 


```{r, include = FALSE}
# Repeated cross validation
rcvSeeds <- setSeeds(method = "repeatedcv", 
                      numbers = K,
                      repeats = Myrepeats, 
                      tunes = 100,
                      seed = Myseed)

# Configure the trainControl argument for cross-validation
K5_CV_seed <- trainControl(method = "cv", 
                           number = K,
                           classProbs = FALSE, 
                           savePredictions = TRUE,
                           seeds = rcvSeeds,
                           allowParallel = TRUE)

```


```{r, cache = FALSE}

# Training this model takes a few minutes
rf_fit <- train(class ~ .,
                 data = train_data,
                 # na.action = na.omit,  # Not needed if missing removed
                  method = "rf",
                  ntree = 100,
                  trControl = K5_CV_seed)
```


```{r, results=TRUE}
rf_fit
```

## Evaluating Model Performance 

We can assess the quality of the model both *in sample*, that is how well the model estimates the data within the training data set  and *out of sample* performance, where we compare predictions based on a validation data set ("*unseen data*"). 

## The *in sample* performance is:

```{r, results=TRUE}
rf_pred <- predict(rf_fit, train_data)
# length(rf_pred)

# Confusion, matrix
 confusionMatrix(rf_pred, train_data$class)
```


We need now further investigation to see if the model generalizes well or if it just has learned from the training data. The high accuracyis a symptom of overfitting.

### Variable importance
```{r}
GISVarImportance<- varImp(rf_fit, scale = FALSE)
plot(GISVarImportance)
```


## The *Out of sample* performance 


```{r, results=TRUE}
rf_pred_Valid <- predict(rf_fit, validation_data)
confusionMatrix(rf_pred_Valid, validation_data$class)
```

### Predicted classification on the validation sample

```{r, results=TRUE}
table(rf_pred_Valid)
```
# Optimizing Random Forests

## Optimizing the combinations of variables 

> A trick is to begin to optimize the random forest on our training process first. 


We therefore begin by tuning the **number of possible variables** (*mtry*)   considered at each split in a decision tree. We do a grid search and compute the accuracy for each value of *mtry* $\in \{0,6\}$.

```{r, cache = TRUE}
# /!\ running this chunk can be long!
tunegrid <- expand.grid(.mtry = (1:6)) 

rf_gridsearch <- train(class ~ ., 
                       data = train_data,
                       method = 'rf',
                       metric = 'accuracy',
                       tuneGrid = tunegrid,
                       ntree = 100)

```



```{r, results=TRUE}
print(rf_gridsearch)
```


```{r mtryANDaccuracy}
ggplot(data=rf_gridsearch$results, aes(Accuracy, x = mtry)) +
  geom_line( colour = SIAP.color)+
  geom_point(colour = "grey")+
  ggtitle(label = "Optimizing the accuracy of the trained random forest model") +
  labs(x = "Nb of variables considered at each split (mtry)" )+
  theme_minimal()
```

```{r mtryANDkappa}
ggplot(data=rf_gridsearch$results, aes(y = Kappa, x = mtry )) +
  geom_line(color= SIAP.red)+
  geom_point(colour = "grey")+
  ggtitle(label = "Optimizing the Kappa of the trained random forest model") +
  labs(x = "Nb of variables considered at each split (mtry)" )+
  theme_minimal()
```

# Predicting


## Importing CSV data to predict from QGIS (Raster image)
We use the data from the whole area (each pixel) and predict on these. 

```{r}
# data directly exported from Jupyter notebook 
QGISDataAllArea <- read.csv("Data/pixels4pred_AllArea_ptsBANDS_wkt.csv")

```








```{r}
# Formatting and removing variables
DataForPrediction <- QGISDataAllArea %>%
  select(-c("cClassSCP", "x", "y", "nClassSCP","SpatialRef", "geometry"))

```

## Predicting using our model 

```{r}
rf_pred_Full <- predict(rf_fit, DataForPrediction)
```


```{r}
DataPredicted <- as.data.frame(cbind(rf_pred_Full, DataForPrediction))
DataPredicted <- DataPredicted %>% 
  mutate(PredictedClass = rf_pred_Full)

```


```{r}
kable(DataPredicted)
```

### Exporting to QGIS

```{r}
# Adding spatial component to Data used for prediction
PredictionForQGIS <- cbind(DataPredicted,QGISDataAllArea %>%
  select(c("cClassSCP", "x", "y", "nClassSCP","SpatialRef", "geometry")) )

# Exporting Data for QGIS
write.csv(PredictionForQGIS, file = "Data/PredictionForQGIS.csv")

```


## Statistics on the prediction

```{r}
# Number of obs. per class
counts <- DataPredicted %>%
  count(PredictedClass) %>%
  arrange(n)  # Sort counts in ascending order

# Reorder the categories based on counts
DataPredicted$PredictedClass<- factor(DataPredicted$PredictedClass, levels = counts$PredictedClass)

# Graphic
DataPredicted %>%
ggplot() + 
  geom_bar(aes(y = PredictedClass), colour="white", fill = SIAP.color) +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))+ 
  labs(x = "", y = "") +
  ggtitle("Land types ")
```


```{r, results=TRUE}
datasummary_skim(DataPredicted$PredictedClass , type = "categorical"  )
```


