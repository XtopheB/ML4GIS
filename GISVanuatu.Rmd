---
title: "Machine Learning for GIS and Land Cover Estimation "
subtitle: "Replicating the Radiant Earth code in R"
author: "Christophe Bontemps (SIAP)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    toc: yes
    toc_float: true
    highlight: tango
    number_sections: no
    theme: lumen
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( message = FALSE, warning = FALSE, results =FALSE, echo = TRUE) 
```

# Installing the packages

```{r packages}
# File management
library(readxl)


# GIS packages
library(raster) ## for reading "RASTER" files
library(rgdal)  ## for reading "shapefiles"
library(sp)     ## for adjusting CRS in 
library(terra)  ## see https://www.neonscience.org/resources/learning-hub/tutorials/image-raster-data-r

# Tidy data management packages
library(dplyr)
library(data.table)
library(modelsummary)

# Model fitting packages for ML
library(rpart)
library(caret)


# Plotting packages
library(ggplot2)
library(RColorBrewer)

# Nice presentation of results
library(knitr)
library(papeR)

# My colors:
SIAP.color <- "#0385a8"
SIAP.red <- "#eb4034"

```


# Importing the Satelite images

There are many data sources freely available with environmental information at a very detailed level. These files are from huge data bases that cover large areas of the word. 




The *raw* files we have downloaded (and which are heavy) can be stored in a project folder for future usage. We collected the images of vanuatu Islands directly from [Copernicus website](https://browser.dataspace.copernicus.eu/?zoom=7&lat=-17.74869&lng=168.31055&themeId=DEFAULT-THEME&visualizationUrl=https%3A%2F%2Fsh.dataspace.copernicus.eu%2Fogc%2Fwms%2Fa91f72b5-f393-4320-bc0f-990129bd9e63&datasetId=S2_L2A_CDAS&demSource3D=%22MAPZEN%22&cloudCoverage=30&dateMode=SINGLE)

We focused on only on the island of Malakula, since this is where we downloaded the data from Copernicus
![](https://www.tourismvanuatu.com/images/vanuatu-map-63483.jpg)


## Raster images
We downloaded several raster images  or *.tiff* files. Each file can embed several layers. We downloaded here several *tiff* files.  Here is a listing all the raster files and all their layers    

```{r, results=TRUE}
GISfiles <- list.files(path = "Data/GIS/",
                      recursive=TRUE,
                      pattern = "*.tiff", full.names=TRUE)

length(GISfiles)
GISfiles
  
```

## Features of Sentinel2 data (raster files) 

It is important to ave a look at the metadata to understand the structure of the data (*e.g.* how many layers) and to find some important information on the **coordinate reference system** (crs) used.  
 
```{r, results = TRUE}
# Reading GIS files 
SentinelData1<-raster(GISfiles[1])   
SentinelData2<-raster(GISfiles[2])   
SentinelData3<-raster(GISfiles[3])   

# Information on a specific file from Sentinel data 
SentinelData1
```
One can also plot directly one image. 

```{r}
plot(SentinelData1,
     main = "Raster Image Plot",      # Title of the plot
     axes = FALSE,                      # Display axes
     box = FALSE,                      # Remove box around plot
     )
```
## Stacking all layers
One can also **stack** all the layers in one single object. Here we selected the files that were collected for a single date. 

```{r}
# create raster stack
# Use (https://www.neonscience.org/resources/learning-hub/tutorials/image-raster-data-r)

FullStack <- stack(GISfiles[1:8])
FullStack
                   
```
Next, we read in the different bands that comprise the satellite imagery. Each band refers to a different spectrum:

```{r}
# plot stack
plot(FullStack)
```
In total in our full stack, we have a lot of different bands and **`r nlayers(FullStack)`** layers

```{r}
nlayers(FullStack)
```

but all use the same crs. 
```{r, results=TRUE}
crs(FullStack)
```


```{r}
#Moisture
plot(raster(GISfiles[4]))
```
Youâ€™ll notice that the range of values for each raster varies:
```{r}
res(raster(GISfiles[4]))
```

## Creating differences or indexes
It can be useful and informative to create new raster image by combining (subtracting, dividing) several images. 
For illustration purpose (this index has no meaning), let's create the followwing index

$$
Myindex  = \frac{Band_4 - Band_5}{Band_4}
$$
```{r}
Myindex <- (raster(GISfiles[4]) - raster(GISfiles[5]))/(raster(GISfiles[4]))
plot(Myindex)
```

The result is a very different image. This is how GIS and EO specialist create Normalized Difference Vegetation Index (NDVI) and other indexes. 



```{r eval=FALSE, include=FALSE}
as(Myindex, "SpatialPixelsDataFrame") %>% 
  as.data.frame() %>%
  ggplot(data = .) +
  geom_tile(aes(x = x, y = y, fill = layer)) +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.background = element_blank(),
        panel.grid.minor = element_blank()) +
  labs(title = "Index plotted with ggplot", 
       x = " ", 
       y = " ") +
  scale_fill_gradient(high = "#CEE50E", 
                      low = "#087F28",
                      name = "Myindex")
```



### (failed) Attempts to download the data directly 
> This does not work well...

```{r}
# Extracting directly from Copenicus (Not really working)
# devtools::install_github("16EAGLE/getSpatialData")

library(getSpatialData)
```

### Area of interest

```{r}
myaoi <- matrix(data = c(22.85, 45.93,  # Upper left corner
                       22.95, 45.93,  # Upper right corner
                       22.95, 45.85,  # Bottom right corner
                       22.85, 45.85,  # Bottom left corner
                       22.85, 45.93), # Upper left corner - closure
              ncol = 2, byrow = TRUE)
#set_aoi(myaoi)
Vanuatu_aoi <- matrix(data = c(166, -13,  # Upper left corner
                               170, -13,  # Upper right corner
                               170, -20,  # Bottom right corner
                               166, -20,  # Bottom left corner
                               166, -13), # Upper left corner - closure
              ncol = 2, byrow = TRUE)

 set_aoi(Vanuatu_aoi)

```

```{r}
view_aoi()
```


```{r}
# Set login credentials
login_CopHub(username = "christophe.bontemps@un.org", password = "COPERziggypop1!")
Sproducts <- get_products("sentinel")
```

```{r}
# Set the archive directory (where the raw data will be downloaded)
# set_archive("./data")
```

```{r}

# product_names <- getSentinel_products()
# product_names
# records <- getSentinel_records(time_range = c("2021-05-15", "2021-05-30"), 
                               # products = product_names[6] )
                                           
                          
```


# Importing Forest inventory data

Radiant Earth and the Vanuatu Department of Forests collected the data set that will serve for estimating the ML model. 


## Loading the data
The data set is an excel file with geographical information. It is not ba GIS file, but tabular data with *x* (longitude) and *Y* (latitude) coordinates

```{r}
Observations <- read_excel("Data/5. sample_plot_measurements.xlsx")
nrow(Observations)
```

> There are some errors in the code 

For example: the latitude (Y) of Port-Vila is -17.734818 and its longitude (X) is 168.322021, so in our data set we should have negative Xs and positve Ys in the dataset. Also the values should be of the same order. 

After cleaning and rearranging the data (coding errors), we can use a clean data set. See Blanca's demo or Radiant Earth code (but that part is not running on Google colab)  


```{r}
# Clean <- read_excel("Data/modified_dataset.xlsx")
# nrow(Clean)
```

## Fixing data issues

### Swapping back latitude (Ys) and longitude (Xs) 

First we remove NAs and exchange latitude and longitudes that were apparently swapped. 
```{r}
Temp <- Observations %>%
  filter(!is.na(general_information_coordinate_of_the_center_x )) %>%
  mutate( swap = ifelse( general_information_coordinate_of_the_center_y > 19, "yes", "no"), 
          latitude = ifelse(swap == "yes", general_information_coordinate_of_the_center_x,
                            general_information_coordinate_of_the_center_y), 
          longitude = ifelse(swap == "yes", general_information_coordinate_of_the_center_y,
                            general_information_coordinate_of_the_center_x))  

```

We have now `r nrow(Temp)` observations (removing `r length(which(is.na(Observations$general_information_coordinate_of_the_center_x ) )) ` NAs )

### Inputing negative values for latitude

```{r}
Clean <- Temp %>%
  mutate(latitude = ifelse(latitude >0, -latitude, latitude))%>%
  select( !c(general_information_coordinate_of_the_center_x,
             general_information_coordinate_of_the_center_y,
             swap,
             Excluded, sample_plot_nr ))
```

> **TBC**, the cleaning can be quite time consumming. 

# Satellite data extraction

We are now  in possession of two sources of geospatial information
- The **satellite images** (raster files from sentinel2)
- The **primary dataset** from forest inventory (points)

We need to match the two information and create a dataset that will be used for our model. 

```{r}
# Main plot using the plotting function of raster package

plot(raster(GISfiles[8]) ,
     main="Map with sample points on a raster image", 
     sub="Malakula  Island") 

# We can add points for each cluster location on this map
points(x= Clean$longitude,
       y= Clean$latitude,
       type="p", col = "red", 
       cex=0.3, 
       pch=21, 
       bg=1)
```

> Alternatively, we could use the data set cleaned from the jupyter notebook (TODO)

```{r}
# library(jsonlite)
# 
# Clean_json_data <- read_json("Data/gdf.json")
# Clean <- as.data.frame(Clean_json_data)


```



## Merging sample points with raster information

The sample point data set needs to be merged with raster images. The resulting file will be of tabular form with, for each observation (points here) the land cover classification (from original sample) and features (variables) coming from the different raster images (bands) that will be considered as potential explanatory variables. 


###  Converting points to spatial format


```{r}
# https://uw-madison-datascience.github.io/r-raster-vector-geospatial/

# First we create the vector with only long and latitude
xy <- cbind(Clean$longitude, Clean$latitude)

# Transform this  into Spatial Vector 
Spatialxy <- vect(xy, crs= "+proj=longlat +datum=WGS84")
```

### Extracting information from raster images 

Now, we have aligned our spatial objects (points and raster images) we can extract information from the satellite images.  At each point in our sample point data set, we will extract the information for each layer of the raster images (that are stored in a *stack*). There are several options for extracting the information. We choose to take the mean of all values in a radius of 20 meters around each points: 

![](https://uw-madison-datascience.github.io/r-raster-vector-geospatial/images/BufferCircular.png)

This may not be necessary, and could be refined later. 

```{r}
# Converting raster Stack object from "raster" package to "terra" package format
# Thanks to Thibault Laurent (TSE) for that trick 

FullStack2 <- rast(FullStack)

# Finally, extract values from raster at the points
DataExtracted <-terra::extract(x= FullStack2, 
                       y = Spatialxy, 
                       buffer= 20, 
                       fun = median, 
                       na.rm = TRUE, 
                       df = TRUE)
                     
DataForModel <- cbind(Clean,DataExtracted )

```


```{r}
# Removing the prefix to ease the reading of the variables 
 colnames(DataForModel) <- gsub('X2021.05.27.00_00_2021.06.27.23_59_Sentinel.2_L2A_','',colnames(DataForModel))
```


A bit of cleaning of the data set that will be used for the ML model. We have so far **`r nrow(DataForModel)`** observations

### Renaming satelite layers names 
 
```{r}
# Select variables from satellite 

Important_var <- DataForModel %>%
  select(starts_with("B"), starts_with("E"),starts_with("N")) %>% 
  colnames()
```


### Removing points with missing information

Many points of our primary dataset were outside the range of our satellite image area of interest (AOI). As a consequence, no information was extracted from the raster files and we may have many missing for the explanatory variables.  

```{r}
# Remove rows with missing values in the selected variables
DataForModel <- DataForModel[complete.cases(DataForModel[Important_var]), , drop = FALSE]
```


> Our dataset for model is now ready. It has a tabular form (**`r nrow(DataForModel)`** observations), each row being a point in the primary dataset. The variables are all information from the primary dataset +  all features from each layer of the Raster images at the selected points. 


# Machine Learning models

## Data Exploration 
 Now, We only have  **`r nrow(DataForModel)`** observations, which is really small...
 
 Still we can do a bit of descriptive analysis

```{r, results= TRUE }
datasummary_skim(DataForModel )
```


## Definition of the classification (dependent) variable 

We have 3 different  levels of classification as described in the NFI codelist. 

```{r , results= TRUE }
datasummary_skim(DataForModel , type = "categorical"  )
```


At the Vegetation Type level 1 , there is a simple distinction between forest (1) and non-Forest (2)

```{r,  results= TRUE }
datasummary_skim(DataForModel$vegetation_type_l1 , type = "categorical"  )
```
We'll start simply with that variable to predict. We need to convert to a factor for the analysis and select the variables (features) that could predict that value.

```{r}
DataForModel$Class1 <- as.factor(DataForModel$vegetation_type_l1)

DataForModel <- DataForModel %>%
  select( "Class1", all_of(Important_var))


```


There is a  **class imbalance** in the target variable *Class1* that we need to keep in mind.  

## Train and Validation data sets

```{r }
# Splits data into training and testing sets
set.seed(2512)
# We'll randomly pick some observation from the full data set

trainIndex <- createDataPartition(DataForModel$Class1, p = .8, 
                                  list = FALSE, 
                                  times = 1)

# Creating the two data sets: 
train_data <- DataForModel[ trainIndex,]
validation_data  <- DataForModel[-trainIndex,]

```

It is important to have a look at our train data set and check if it has the same characteristics. 

```{r,  results= TRUE }
datasummary_skim(train_data$Class1 , type = "categorical"  )
```


# Random Forest

Random Forest is *bagging* (*bootstrap aggregation*) method. During training a bootstrap sample is drawn from the training data together with a subset of the total amount of variables, we then pick the best available variable to split the tree into two daughter nodes repeatedly until we reach a stopping criterion. This process is repeated until we have trained as many individual decision trees as we want. We can then combine the output of the decision trees into one final output.

In the case of regression we average the output over all the decision trees, whereas in classification our final output will be based on the **majority vote** of the individual decision trees. 

## Fitting the Random Forest


```{r, include = FALSE, echo = FALSE}
# function to set up random seeds when running on several cores 
#  (you do not need to understand that)

setSeeds <- function(method = "cv", numbers = 1, repeats = 1, tunes = NULL, seed = 1237) {
  #B is the number of resamples and integer vector of M (numbers + tune length if any)
  B <- if (method == "cv") numbers
  else if(method == "repeatedcv") numbers * repeats
  else NULL
  
  if(is.null(length)) {
    seeds <- NULL
  } else {
    set.seed(seed = seed)
    seeds <- vector(mode = "list", length = B)
    seeds <- lapply(seeds, function(x) sample.int(n = 1000000, size = numbers + ifelse(is.null(tunes), 0, tunes)))
    seeds[[length(seeds) + 1]] <- sample.int(n = 1000000, size = 1)
  }
  # return seeds
  seeds
}

```



```{r, include = FALSE}
# Repeated cross validation
rcvSeeds <- setSeeds(method = "repeatedcv", 
                      numbers = 5, repeats = 5, 
                      tunes = 100, seed = 777)

# Configure the trainControl argument for cross-validation
K5_CV_seed <- trainControl(method = "cv", number = 5, classProbs = FALSE, 
                           savePredictions = TRUE, seeds = rcvSeeds,
                           allowParallel = TRUE)

```

We begin with fitting a Random Forest model using cross-validation on the *Training* data set 

```{r, cache = TRUE}
set.seed(2512)

# Training this model takes a few minutes
rf_fit <- train(Class1 ~ .,
                  data = train_data,
                  method = "rf",
                  trControl = K5_CV_seed)
```

```{r, results=TRUE}
rf_fit
```

## Evaluating Model Performance 

We can also assess the decision trees both *in sample* and *out of sample* performance. 

#### The *in sample* performance is:

```{r, results=TRUE}
rf_pred <- predict(rf_fit, train_data)
confusionMatrix(rf_pred, train_data$Class1, positive = "1")
```

We need now further investigation to see if the model generalizes well or if it just has learned the training data. 

#### The *Out of sample* performance:


```{r, results=TRUE}
rf_pred <- predict(rf_fit, validation_data)
confusionMatrix(rf_pred, validation_data$Class1, positive = "1")
```


The out of sample performance is obviously lower, the sensitivity and kappa lower than before but the difference is not that important.  We may still try to improve by optimizing some of the parameters. 

# Optimizing Random Forests


## Optimizing the combinations of variables 

> A trick is to begin to optimize the random forest on our training process first. 
This may end up saving us a lot of training time. 

We therefore begin by tuning the **number of possible variables** considered at each split in a decision tree by tuning the *mtry* hyperparameter. We do a grid search and compute the accuracy for each value of *mtry* $\in \{0,8\}$.

```{r, cache = TRUE}
# /!\ running this chunk can be long!
tunegrid <- expand.grid(.mtry = (1:8)) 

rf_gridsearch <- train(Class1 ~ ., 
                       data = train_data,
                       method = 'rf',
                       metric = 'accuracy',
                       tuneGrid = tunegrid,
                       ntree = 100)

```


```{r, results=TRUE}
print(rf_gridsearch)
```


```{r mtryANDaccuracy}
ggplot(data=rf_gridsearch$results, aes(Accuracy, x = mtry)) +
  geom_line( colour = SIAP.color)+
  geom_point(colour = "grey")+
  ggtitle(label = "Optimizing the accuracy of the trained random forest model") +
  labs(x = "Nb of variables considered at each split (mtry)" )+
  theme_minimal()
```

```{r mtryANDkappa}
ggplot(data=rf_gridsearch$results, aes(y = Kappa, x = mtry )) +
  geom_line(color= SIAP.red)+
  geom_point(colour = "grey")+
  ggtitle(label = "Optimizing the Kappa of the trained random forest model") +
  labs(x = "Nb of variables considered at each split (mtry)" )+
  theme_minimal()
```



```{r, exit}
knitr::knit_exit()
```


****

### Correlation plot {-}


```{r }
library(ggcorrplot)

# We compute the correlation matrix of the covariates
corr_coef<-cor(data.agg[, c(3:10)],use = "p")
#And then plot it with nice options 
ggcorrplot(corr_coef, 
           type = "lower",         # lower triangle of the matrix only
           hc.order = TRUE,        # variable sorted from highest to lowest
           outline.col = "white",  #Color options
           lab = TRUE)

```

## 4.2 Logistic regression

```{r}
# We use the dhsdataMerge function to merge the survey data (individuals)
# with all the Geo-covariate extracted at the cluster level
DataMerged1<-dhsdataMerge(merged1)

# We need to have a factor variable and not directly Before15 (that is numeric here)  
DataMerged1$I_Before15 <- as.factor(DataMerged1$Before15)

# Education is a factor variable
DataMerged1$Education <- as.factor(DataMerged1$Education)
# DataMerged1 <- DataMerged1 %>%                    # defining the reference category
#   mutate(Education = relevel(Education, "0-No"))
# 

# We change the unit of Aridity here 
DataMerged1$Aridity2015 <- DataMerged1$Aridity2015 * 10^8

# Defining the variables of the model
Y<-"I_Before15"               # Response variable
XCovars <- c(15, 17, 57:64)   # age+education+GIS

formula_string<- paste(Y, paste(colnames(DataMerged1)[XCovars], collapse=" + "), sep="~")
print(paste(" Regression formula: ",formula_string))

```
### Results as in **Figure 7**

```{r, results='asis'}
# Logistics Regression
glm.fit <- glm(formula_string, data = DataMerged1, family = binomial)

# Nice printing of the results (using paper and knitr packages)
pretty_lm2 <- prettify(summary(glm.fit))
kable(pretty_lm2, digits = 3)

```


### Confusion Matrix {-}
```{r, results=TRUE }
library("regclass")
confusion_matrix(glm.fit)
```


### Visual representation of the logistic model{-} 


```{r visreg}
library(visreg)
library(ggpubr)

# Probabilities of married before 15 wrt 
p.age <- visreg(glm.fit, "Age", scale="response", rug=0,  # for rugs =2
       xlab="Age",
       ylab="P(Before15=1)", gg=TRUE) + 
  ylim(0,1) +theme_minimal()

p.education <- visreg(glm.fit, "Education", scale="response", rug=0,
       xlab="Education",
       ylab="P(Before15=1)", gg=TRUE) + 
  ylim(0,1) + theme_minimal() + 
 theme(axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust=1,
                                   size=7))


p.aridity <- visreg(glm.fit, "Aridity2015", scale="response", rug=0,
       xlab="Aridity level (2015)",
       ylab="P(Before15=1)", gg=TRUE) + 
  ylim(0,1) +theme_minimal()

p.income <- visreg(glm.fit, "aIncome2013", scale="response", rug=0,
       xlab=" Estimated income (in $ 2013)",
       ylab="P(Before15=1)", gg=TRUE) + 
  ylim(0,1) +theme_minimal()


figure <- ggarrange( p.age, p.education, p.aridity, p.income,
                    #labels = c("Edudation", "Age",  "Aridity (2015)", ""),
                    ncol = 2, nrow = 2)
figure
```


# 4.3 Random Forests  
 
 
```{r RF, cache = TRUE}
set.seed(888)               # set random seed so we can reproduce the result
myRandomForest<-randomForest(as.formula(formula_string),
                             data = DataMerged1,
                             importance = TRUE,
                             maxnodes=25,
                             ntree=1000,
                             type="classification",
                             na.action = na.roughfix)
```

### Accuracy rate and confusion Matrix {-}

```{r, results = TRUE}
myRandomForest

```


### Variable importance plot as in Figure 12 

```{r}
varImpPlot(myRandomForest, 
           type = 1,
           main =" Importance Plot for Random Forest Model")
```



